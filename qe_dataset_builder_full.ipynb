{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Enhanced QE Dataset Builder (Full Version with Stronger Degradation)\n",
        "# All comments in English.\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import re\n",
        "import csv\n",
        "\n",
        "# Base paths\n",
        "BASE_DIR = os.getcwd()\n",
        "DATASET_DIR = BASE_DIR + '\\\\dataset'\n",
        "\n",
        "# Merge text files inside a folder\n",
        "def merge_text_files(folder_path, output_file=\"merged.txt\", encoding=\"utf-8\"):\n",
        "    with open(output_file, \"w\", encoding=encoding) as outfile:\n",
        "        for filename in sorted(os.listdir(folder_path)):\n",
        "            if filename.lower().endswith(\".txt\"):\n",
        "                file_path = os.path.join(folder_path, filename)\n",
        "                with open(file_path, \"r\", encoding=encoding) as infile:\n",
        "                    content = infile.read().strip()\n",
        "                    outfile.write(content + \"\\n\")\n",
        "    print(f\"[OK] All text files have been combined into '{output_file}'.\")\n",
        "\n",
        "# Prepare source and target merged text files\n",
        "source = DATASET_DIR + '\\\\ko\\\\'\n",
        "src_path = BASE_DIR + '\\\\source.txt'\n",
        "if not os.path.exists(src_path):\n",
        "    merge_text_files(source, output_file='source.txt')\n",
        "\n",
        "target = DATASET_DIR + '\\\\en\\\\'\n",
        "tgt_path = BASE_DIR + '\\\\target.txt'\n",
        "if not os.path.exists(tgt_path):\n",
        "    merge_text_files(target, output_file='target.txt')\n",
        "\n",
        "# Argument parser\n",
        "p = argparse.ArgumentParser(description=\"Build synthetic QE dataset CSV\")\n",
        "p.add_argument(\"--source\", default=\"source.txt\")\n",
        "p.add_argument(\"--target\", default=\"target.txt\")\n",
        "p.add_argument(\"--out\", default=\"dataset.csv\")\n",
        "p.add_argument(\"--seed\", type=int, default=42)\n",
        "p.add_argument(\"--no-perfect\", action=\"store_true\")\n",
        "args, _ = p.parse_known_args()\n",
        "\n",
        "# File existence check\n",
        "if not os.path.exists(args.source) or not os.path.exists(args.target):\n",
        "    raise FileNotFoundError(\"source.txt / target.txt not found.\")\n",
        "\n",
        "# Read lines\n",
        "def read_lines(path: str):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [ln.rstrip(\"\\n\\r\") for ln in f.readlines()]\n",
        "\n",
        "srcs = read_lines(args.source)\n",
        "tgts = read_lines(args.target)\n",
        "\n",
        "if len(srcs) != len(tgts):\n",
        "    raise ValueError(\"Source and target file line counts do not match.\")\n",
        "\n",
        "# Regex helpers\n",
        "_WORD_RE = re.compile(r\"[^\\W\\d_]+\", re.UNICODE)\n",
        "_NUM_RE = re.compile(r\"\\b\\d{1,4}\\b\")\n",
        "_AMPM_RE = re.compile(r\"\\b(am|pm|a\\.m\\.|p\\.m\\.)\\b\", re.IGNORECASE)\n",
        "\n",
        "MONTHS = [\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
        "          \"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]\n",
        "DAYS = [\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\"]\n",
        "\n",
        "AUX = [\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
        "       \"do\",\"does\",\"did\",\"have\",\"has\",\"had\",\n",
        "       \"can\",\"could\",\"will\",\"would\",\"shall\",\"should\",\"may\",\"might\",\"must\"]\n",
        "\n",
        "STOP_LITE = {\"a\",\"an\",\"the\",\"to\",\"of\",\"in\",\"on\",\"for\",\"and\",\"or\",\"but\",\"so\",\n",
        "             \"as\",\"at\",\"by\",\"with\",\"from\",\"than\",\"then\",\"there\",\"here\"}\n",
        "\n",
        "ALPHABET = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "FILLERS = [\"actually\", \"really\", \"kind of\", \"maybe\", \"sort of\", \"in fact\"]\n",
        "\n",
        "# Case preservation\n",
        "def _preserve_case(src_token: str, repl: str) -> str:\n",
        "    if src_token.isupper(): return repl.upper()\n",
        "    if src_token[0].isupper(): return repl.capitalize()\n",
        "    return repl\n",
        "\n",
        "# Replace one vocabulary item\n",
        "def replace_one_of_set(text: str, vocab: list[str]) -> str:\n",
        "    lowered = text.lower()\n",
        "    candidates = [w for w in vocab if re.search(rf\"\\b{re.escape(w)}\\b\", lowered)]\n",
        "    if not candidates:\n",
        "        return text\n",
        "    target = random.choice(candidates)\n",
        "    others = [w for w in vocab if w != target]\n",
        "    if not others:\n",
        "        return text\n",
        "    repl = random.choice(others)\n",
        "\n",
        "    def _sub(m):\n",
        "        tok = m.group(0)\n",
        "        return _preserve_case(tok, repl)\n",
        "\n",
        "    return re.sub(rf\"\\b{re.escape(target)}\\b\", _sub, text, count=1, flags=re.IGNORECASE)\n",
        "\n",
        "# Mutate numbers\n",
        "def mutate_numbers(text: str) -> str:\n",
        "    def _mut(m):\n",
        "        n = int(m.group(0))\n",
        "        if n <= 2:\n",
        "            return str(n + 1)\n",
        "        return str(n - 1 if random.random() < 0.5 else n + 1)\n",
        "    return _NUM_RE.sub(_mut, text, count=random.randint(1, 2))\n",
        "\n",
        "# Flip AM/PM\n",
        "def flip_ampm(text: str) -> str:\n",
        "    def _flip(m):\n",
        "        v = m.group(0)\n",
        "        low = v.lower()\n",
        "        alt = \"pm\" if \"am\" in low else \"am\"\n",
        "        if \".\" in low:\n",
        "            alt = alt[0] + \".m.\"\n",
        "        return _preserve_case(v, alt)\n",
        "    return _AMPM_RE.sub(_flip, text, count=1)\n",
        "\n",
        "# Insert or remove 'not'\n",
        "def insert_or_remove_not(text: str) -> str:\n",
        "    if re.search(r\"\\bnot\\b\", text, flags=re.IGNORECASE) and random.random() < 0.5:\n",
        "        return re.sub(r\"\\bnot\\b\",\"\", text, count=1, flags=re.IGNORECASE).replace(\"  \",\" \")\n",
        "    for aux in AUX:\n",
        "        m = re.search(rf\"\\b{aux}\\b\", text, flags=re.IGNORECASE)\n",
        "        if m:\n",
        "            i = m.end()\n",
        "            return text[:i] + \" not\" + text[i:]\n",
        "    return text\n",
        "\n",
        "# Drop content words\n",
        "def drop_content_words(text: str, drop_ratio=(0.15, 0.35)) -> str:\n",
        "    tokens = text.split()\n",
        "    if not tokens: return text\n",
        "    k = random.uniform(*drop_ratio)\n",
        "    kept = []\n",
        "    for t in tokens:\n",
        "        plain = re.sub(r\"[^\\w]\", \"\", t)\n",
        "        is_word = bool(_WORD_RE.fullmatch(plain)) and plain\n",
        "        if is_word and len(plain) > 3 and plain.lower() not in STOP_LITE:\n",
        "            if random.random() < k:\n",
        "                continue\n",
        "        kept.append(t)\n",
        "    out = \" \".join(kept)\n",
        "    return out if out.strip() else text\n",
        "\n",
        "# Shuffle chunks\n",
        "def shuffle_chunks(text: str) -> str:\n",
        "    parts = re.split(r\"(,|;| and | but )\", text)\n",
        "    if len(parts) < 3:\n",
        "        toks = text.split()\n",
        "        random.shuffle(toks)\n",
        "        return \" \".join(toks)\n",
        "\n",
        "    chunks, cur = [], \"\"\n",
        "    for p in parts:\n",
        "        cur += p\n",
        "        if p in {\",\",\";\",\" and \",\" but \"}:\n",
        "            chunks.append(cur.strip())\n",
        "            cur = \"\"\n",
        "    if cur.strip(): chunks.append(cur.strip())\n",
        "    random.shuffle(chunks)\n",
        "    return \" \".join(chunks)\n",
        "\n",
        "# Typo word\n",
        "def _typo_word(word: str) -> str:\n",
        "    core = re.sub(r\"^\\W+|\\W+$\", \"\", word)\n",
        "    if len(core) < 4:\n",
        "        return word\n",
        "    i = random.randrange(1, len(core) - 1)\n",
        "    old = core[i]\n",
        "    candidates = [c for c in ALPHABET if c != old.lower()]\n",
        "    if not candidates: return word\n",
        "    new = random.choice(candidates)\n",
        "    if old.isupper(): new = new.upper()\n",
        "    out = core[:i] + new + core[i+1:]\n",
        "    prefix = word[:word.find(core)] if core in word else \"\"\n",
        "    suffix = word[word.find(core)+len(core):] if core in word else \"\"\n",
        "    return prefix + out + suffix\n",
        "\n",
        "def introduce_typos(text: str, prob=0.3) -> str:\n",
        "    tokens = text.split()\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        if random.random() < prob:\n",
        "            out.append(_typo_word(t))\n",
        "        else:\n",
        "            out.append(t)\n",
        "    return \" \".join(out)\n",
        "\n",
        "# Insert noise token\n",
        "def insert_noise_token(text: str):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) < 3: return text\n",
        "    idx = random.randrange(1, len(tokens))\n",
        "    if random.random() < 0.5:\n",
        "        tokens.insert(idx, random.choice(FILLERS))\n",
        "    else:\n",
        "        tokens.insert(idx, tokens[idx-1])\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Truncate tail\n",
        "def truncate_tail(text: str, min_words=5, max_drop_ratio=0.4):\n",
        "    tokens = text.split()\n",
        "    n = len(tokens)\n",
        "    if n <= min_words+2: return text\n",
        "    max_drop = int(n * max_drop_ratio)\n",
        "    cut = random.randint(min_words, n - max_drop)\n",
        "    return \" \".join(tokens[:cut])\n",
        "\n",
        "# Corrupt punctuation\n",
        "def corrupt_punctuation(text: str):\n",
        "    if not re.search(r\"[,.!?]\", text): return text\n",
        "    mode = random.choice([\"drop\",\"swap\"])\n",
        "    if mode == \"drop\":\n",
        "        return re.sub(r\"[,.!?]\",\"\", text, count=random.randint(1,3))\n",
        "    else:\n",
        "        t = text.replace(\"...\",\".\")\n",
        "        t = re.sub(r\"\\?\",\".\", t)\n",
        "        return re.sub(r\"\\.\",\",\", t, count=1)\n",
        "\n",
        "# Strong degrade function\n",
        "def degrade_mid(text: str) -> str:\n",
        "    ops = [\n",
        "        lambda s: replace_one_of_set(s, MONTHS),\n",
        "        lambda s: replace_one_of_set(s, DAYS),\n",
        "        mutate_numbers,\n",
        "        flip_ampm,\n",
        "        insert_or_remove_not,\n",
        "        drop_content_words,\n",
        "        shuffle_chunks,\n",
        "        introduce_typos,\n",
        "        insert_noise_token,\n",
        "        corrupt_punctuation,\n",
        "        truncate_tail,\n",
        "    ]\n",
        "\n",
        "    random.shuffle(ops)\n",
        "    n_tokens = len(text.split())\n",
        "    base = 4 if n_tokens < 10 else 5\n",
        "    extra = 1 if n_tokens < 20 else 2\n",
        "    num_ops = min(len(ops), base + random.randint(0,extra))\n",
        "\n",
        "    out = text\n",
        "    for fn in ops[:num_ops]:\n",
        "        out = fn(out)\n",
        "\n",
        "    if out.strip() == text.strip():\n",
        "        out = drop_content_words(out, drop_ratio=(0.30,0.50))\n",
        "\n",
        "    return re.sub(r\"\\s+\",\" \", out).strip()\n",
        "\n",
        "# Build dataset\n",
        "def make_dataset(sources, targets, include_perfect=True, seed=42):\n",
        "    random.seed(seed)\n",
        "    n = len(sources)\n",
        "    rows = []\n",
        "\n",
        "    perm = list(range(n))\n",
        "    random.shuffle(perm)\n",
        "    for i in range(n):\n",
        "        if perm[i] == i:\n",
        "            j = (i+1) % n\n",
        "            perm[i], perm[j] = perm[j], perm[i]\n",
        "\n",
        "    for i in range(n):\n",
        "        src = sources[i].strip()\n",
        "        good = targets[i].strip()\n",
        "\n",
        "        if include_perfect:\n",
        "            rows.append([src, good, 100])\n",
        "\n",
        "        mid = degrade_mid(good)\n",
        "        if len(mid.split()) <= max(2, len(good.split())//4):\n",
        "            mid = degrade_mid(good)\n",
        "        rows.append([src, mid, 50])\n",
        "\n",
        "        wrong = targets[perm[i]].strip()\n",
        "        rows.append([src, wrong, 0])\n",
        "\n",
        "    return rows\n",
        "\n",
        "rows = make_dataset(srcs, tgts, include_perfect=not args.no_perfect, seed=args.seed)\n",
        "\n",
        "# Write CSV\n",
        "def write_csv(rows, out_path: str):\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"SOURCE\",\"TARGET\",\"SCORE\"])\n",
        "        for r in rows:\n",
        "            w.writerow(r)\n",
        "\n",
        "write_csv(rows, args.out)\n",
        "\n",
        "print(\"Dataset generation complete.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
