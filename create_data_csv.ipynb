{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19219a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\pockg\\\\OneDrive\\\\Desktop\\\\Hojumoney\\\\DEV\\\\model-scoring-translating\\\\.venv\\\\Scripts\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca040a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.2.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: transformers>=4.44.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (4.57.3)\n",
      "Requirement already satisfied: datasets>=2.21.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (4.4.1)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.5.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (1.7.2)\n",
      "Requirement already satisfied: sentencepiece>=0.1.99 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.2.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (2.3.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from transformers>=4.44.0->-r requirements.txt (line 2)) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from transformers>=4.44.0->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from transformers>=4.44.0->-r requirements.txt (line 2)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from transformers>=4.44.0->-r requirements.txt (line 2)) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from transformers>=4.44.0->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from transformers>=4.44.0->-r requirements.txt (line 2)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from transformers>=4.44.0->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from transformers>=4.44.0->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from datasets>=2.21.0->-r requirements.txt (line 3)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from datasets>=2.21.0->-r requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from datasets>=2.21.0->-r requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from datasets>=2.21.0->-r requirements.txt (line 3)) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from datasets>=2.21.0->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from datasets>=2.21.0->-r requirements.txt (line 3)) (0.70.18)\n",
      "Requirement already satisfied: psutil in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from accelerate>=0.34.0->-r requirements.txt (line 4)) (7.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from scikit-learn>=1.5.0->-r requirements.txt (line 5)) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from scikit-learn>=1.5.0->-r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from scikit-learn>=1.5.0->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0->-r requirements.txt (line 3)) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=2.21.0->-r requirements.txt (line 3)) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=2.21.0->-r requirements.txt (line 3)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=2.21.0->-r requirements.txt (line 3)) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets>=2.21.0->-r requirements.txt (line 3)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.21.0->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from requests->transformers>=4.44.0->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from requests->transformers>=4.44.0->-r requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.2.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers>=4.44.0->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from jinja2->torch>=2.2.0->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from pandas->datasets>=2.21.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from pandas->datasets>=2.21.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from pandas->datasets>=2.21.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0->-r requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0->-r requirements.txt (line 3)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0->-r requirements.txt (line 3)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0->-r requirements.txt (line 3)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.21.0->-r requirements.txt (line 3)) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\pockg\\onedrive\\desktop\\hojumoney\\dev\\model-scoring-translating\\.venv\\lib\\site-packages (from anyio->httpx<1.0.0->datasets>=2.21.0->-r requirements.txt (line 3)) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7e52ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- -----------\n",
      "accelerate              1.12.0\n",
      "aiohappyeyeballs        2.6.1\n",
      "aiohttp                 3.13.2\n",
      "aiosignal               1.4.0\n",
      "anyio                   4.11.0\n",
      "asttokens               3.0.1\n",
      "attrs                   25.4.0\n",
      "certifi                 2025.11.12\n",
      "charset-normalizer      3.4.4\n",
      "colorama                0.4.6\n",
      "comm                    0.2.3\n",
      "datasets                4.4.1\n",
      "debugpy                 1.8.17\n",
      "decorator               5.2.1\n",
      "dill                    0.4.0\n",
      "executing               2.2.1\n",
      "filelock                3.20.0\n",
      "frozenlist              1.8.0\n",
      "fsspec                  2025.10.0\n",
      "h11                     0.16.0\n",
      "httpcore                1.0.9\n",
      "httpx                   0.28.1\n",
      "huggingface-hub         0.36.0\n",
      "idna                    3.11\n",
      "ipykernel               7.1.0\n",
      "ipython                 9.7.0\n",
      "ipython_pygments_lexers 1.1.1\n",
      "jedi                    0.19.2\n",
      "Jinja2                  3.1.6\n",
      "joblib                  1.5.2\n",
      "jupyter_client          8.6.3\n",
      "jupyter_core            5.9.1\n",
      "MarkupSafe              3.0.3\n",
      "matplotlib-inline       0.2.1\n",
      "mpmath                  1.3.0\n",
      "multidict               6.7.0\n",
      "multiprocess            0.70.18\n",
      "nest-asyncio            1.6.0\n",
      "networkx                3.6\n",
      "numpy                   2.3.5\n",
      "packaging               25.0\n",
      "pandas                  2.3.3\n",
      "parso                   0.8.5\n",
      "pip                     24.0\n",
      "platformdirs            4.5.0\n",
      "prompt_toolkit          3.0.52\n",
      "propcache               0.4.1\n",
      "psutil                  7.1.3\n",
      "pure_eval               0.2.3\n",
      "pyarrow                 22.0.0\n",
      "Pygments                2.19.2\n",
      "python-dateutil         2.9.0.post0\n",
      "pytz                    2025.2\n",
      "PyYAML                  6.0.3\n",
      "pyzmq                   27.1.0\n",
      "regex                   2025.11.3\n",
      "requests                2.32.5\n",
      "safetensors             0.7.0\n",
      "scikit-learn            1.7.2\n",
      "scipy                   1.16.3\n",
      "sentencepiece           0.2.1\n",
      "setuptools              65.5.0\n",
      "six                     1.17.0\n",
      "sniffio                 1.3.1\n",
      "stack-data              0.6.3\n",
      "sympy                   1.14.0\n",
      "threadpoolctl           3.6.0\n",
      "tokenizers              0.22.1\n",
      "torch                   2.9.1\n",
      "tornado                 6.5.2\n",
      "tqdm                    4.67.1\n",
      "traitlets               5.14.3\n",
      "transformers            4.57.3\n",
      "typing_extensions       4.15.0\n",
      "tzdata                  2025.2\n",
      "urllib3                 2.5.0\n",
      "wcwidth                 0.2.14\n",
      "xxhash                  3.6.0\n",
      "yarl                    1.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fd50dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c:\\\\Users\\\\pockg\\\\OneDrive\\\\Desktop\\\\Hojumoney\\\\DEV\\\\model-scoring-translating',\n",
       " 'c:\\\\Users\\\\pockg\\\\OneDrive\\\\Desktop\\\\Hojumoney\\\\DEV\\\\model-scoring-translating\\\\dataset')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATASET_DIR = BASE_DIR + '\\\\dataset'\n",
    "BASE_DIR, DATASET_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbabc9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] All text files has been combined into 'source.txt'.\n",
      "[OK] All text files has been combined into 'target.txt'.\n"
     ]
    }
   ],
   "source": [
    "def merge_text_files(folder_path, output_file=\"merged.txt\", encoding=\"utf-8\"):\n",
    "    with open(output_file, \"w\", encoding=encoding) as outfile:\n",
    "        for filename in sorted(os.listdir(folder_path)):\n",
    "            if filename.lower().endswith(\".txt\"):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, \"r\", encoding=encoding) as infile:\n",
    "                    content = infile.read().strip()\n",
    "                    outfile.write(content + \"\\n\")\n",
    "    print(f\"[OK] All text files has been combined into '{output_file}'.\")\n",
    "\n",
    "source = DATASET_DIR+'\\\\ko\\\\'\n",
    "src_path = BASE_DIR+'\\\\source.txt'\n",
    "if not os.path.exists(src_path):\n",
    "    merge_text_files(source, output_file='source.txt')\n",
    "else:\n",
    "    print(\"The source file already exists.\")\n",
    "    print(f'- file path: {src_path}')\n",
    "\n",
    "target = DATASET_DIR+'\\\\en\\\\'\n",
    "tgt_path = BASE_DIR+'\\\\target.txt'\n",
    "if not os.path.exists(BASE_DIR+'\\\\target.txt'):\n",
    "    merge_text_files(target, output_file='target.txt')\n",
    "else:\n",
    "    print(\"The target file already exists.\")\n",
    "    print(f'- file path: {tgt_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0271ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = argparse.ArgumentParser(description=\"Build synthetic QE dataset CSV\")\n",
    "p.add_argument(\"--source\", default=\"source.txt\", help=\"UTF-8 Korean source file\")\n",
    "p.add_argument(\"--target\", default=\"target.txt\", help=\"UTF-8 English target file\")\n",
    "p.add_argument(\"--out\", default=\"dataset.csv\", help=\"Output CSV path\")\n",
    "p.add_argument(\"--seed\", type=int, default=42)\n",
    "p.add_argument(\"--no-perfect\", action=\"store_true\", help=\"Do NOT include 100-point rows\")\n",
    "\n",
    "args, _ = p.parse_known_args()\n",
    "if not os.path.exists(args.source) or not os.path.exists(args.target):\n",
    "    raise FileNotFoundError(\"Check the file source.txt / target.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dfa58a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8030, 8030)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_lines(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [ln.rstrip(\"\\n\\r\") for ln in f.readlines()]\n",
    "\n",
    "srcs = read_lines(args.source)\n",
    "tgts = read_lines(args.target)\n",
    "if len(srcs) != len(tgts):\n",
    "    raise ValueError(\"Both files are not matching each other.\")\n",
    "len(srcs), len(tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ccc7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex helpers\n",
    "_WORD_RE = re.compile(r\"[^\\W\\d_]+\", re.UNICODE)\n",
    "_NUM_RE = re.compile(r\"\\b\\d{1,4}\\b\")\n",
    "_AMPM_RE = re.compile(r\"\\b(am|pm|a\\.m\\.|p\\.m\\.)\\b\", re.IGNORECASE)\n",
    "\n",
    "MONTHS = [\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
    "          \"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]\n",
    "DAYS = [\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\"]\n",
    "\n",
    "AUX = [\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
    "       \"do\",\"does\",\"did\",\"have\",\"has\",\"had\",\n",
    "       \"can\",\"could\",\"will\",\"would\",\"shall\",\"should\",\"may\",\"might\",\"must\"]\n",
    "\n",
    "STOP_LITE = {\"a\",\"an\",\"the\",\"to\",\"of\",\"in\",\"on\",\"for\",\"and\",\"or\",\"but\",\"so\",\n",
    "             \"as\",\"at\",\"by\",\"with\",\"from\",\"than\",\"then\",\"there\",\"here\"}\n",
    "\n",
    "ALPHABET = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "FILLERS = [\"actually\", \"really\", \"kind of\", \"maybe\", \"sort of\", \"in fact\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "511ee2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case preservation\n",
    "def _preserve_case(src_token: str, repl: str) -> str:\n",
    "    if src_token.isupper(): return repl.upper()\n",
    "    if src_token[0].isupper(): return repl.capitalize()\n",
    "    return repl\n",
    "\n",
    "# Replace one vocabulary item\n",
    "def replace_one_of_set(text: str, vocab: list[str]) -> str:\n",
    "    lowered = text.lower()\n",
    "    candidates = [w for w in vocab if re.search(rf\"\\b{re.escape(w)}\\b\", lowered)]\n",
    "    if not candidates:\n",
    "        return text\n",
    "    target = random.choice(candidates)\n",
    "    others = [w for w in vocab if w != target]\n",
    "    if not others:\n",
    "        return text\n",
    "    repl = random.choice(others)\n",
    "\n",
    "    def _sub(m):\n",
    "        tok = m.group(0)\n",
    "        return _preserve_case(tok, repl)\n",
    "\n",
    "    return re.sub(rf\"\\b{re.escape(target)}\\b\", _sub, text, count=1, flags=re.IGNORECASE)\n",
    "\n",
    "# Mutate numbers\n",
    "def mutate_numbers(text: str) -> str:\n",
    "    def _mut(m):\n",
    "        n = int(m.group(0))\n",
    "        if n <= 2:\n",
    "            return str(n + 1)\n",
    "        return str(n - 1 if random.random() < 0.5 else n + 1)\n",
    "    return _NUM_RE.sub(_mut, text, count=random.randint(1, 2))\n",
    "\n",
    "# Flip AM/PM\n",
    "def flip_ampm(text: str) -> str:\n",
    "    def _flip(m):\n",
    "        v = m.group(0)\n",
    "        low = v.lower()\n",
    "        alt = \"pm\" if \"am\" in low else \"am\"\n",
    "        if \".\" in low:\n",
    "            alt = alt[0] + \".m.\"\n",
    "        return _preserve_case(v, alt)\n",
    "    return _AMPM_RE.sub(_flip, text, count=1)\n",
    "\n",
    "# Insert or remove 'not'\n",
    "def insert_or_remove_not(text: str) -> str:\n",
    "    if re.search(r\"\\bnot\\b\", text, flags=re.IGNORECASE) and random.random() < 0.5:\n",
    "        return re.sub(r\"\\bnot\\b\",\"\", text, count=1, flags=re.IGNORECASE).replace(\"  \",\" \")\n",
    "    for aux in AUX:\n",
    "        m = re.search(rf\"\\b{aux}\\b\", text, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            i = m.end()\n",
    "            return text[:i] + \" not\" + text[i:]\n",
    "    return text\n",
    "\n",
    "# Drop content words\n",
    "def drop_content_words(text: str, drop_ratio=(0.15, 0.35)) -> str:\n",
    "    tokens = text.split()\n",
    "    if not tokens: return text\n",
    "    k = random.uniform(*drop_ratio)\n",
    "    kept = []\n",
    "    for t in tokens:\n",
    "        plain = re.sub(r\"[^\\w]\", \"\", t)\n",
    "        is_word = bool(_WORD_RE.fullmatch(plain)) and plain\n",
    "        if is_word and len(plain) > 3 and plain.lower() not in STOP_LITE:\n",
    "            if random.random() < k:\n",
    "                continue\n",
    "        kept.append(t)\n",
    "    out = \" \".join(kept)\n",
    "    return out if out.strip() else text\n",
    "\n",
    "# Shuffle chunks\n",
    "def shuffle_chunks(text: str) -> str:\n",
    "    parts = re.split(r\"(,|;| and | but )\", text)\n",
    "    if len(parts) < 3:\n",
    "        toks = text.split()\n",
    "        random.shuffle(toks)\n",
    "        return \" \".join(toks)\n",
    "\n",
    "    chunks, cur = [], \"\"\n",
    "    for p in parts:\n",
    "        cur += p\n",
    "        if p in {\",\",\";\",\" and \",\" but \"}:\n",
    "            chunks.append(cur.strip())\n",
    "            cur = \"\"\n",
    "    if cur.strip(): chunks.append(cur.strip())\n",
    "    random.shuffle(chunks)\n",
    "    return \" \".join(chunks)\n",
    "\n",
    "# Typo word\n",
    "def _typo_word(word: str) -> str:\n",
    "    core = re.sub(r\"^\\W+|\\W+$\", \"\", word)\n",
    "    if len(core) < 4:\n",
    "        return word\n",
    "    i = random.randrange(1, len(core) - 1)\n",
    "    old = core[i]\n",
    "    candidates = [c for c in ALPHABET if c != old.lower()]\n",
    "    if not candidates: return word\n",
    "    new = random.choice(candidates)\n",
    "    if old.isupper(): new = new.upper()\n",
    "    out = core[:i] + new + core[i+1:]\n",
    "    prefix = word[:word.find(core)] if core in word else \"\"\n",
    "    suffix = word[word.find(core)+len(core):] if core in word else \"\"\n",
    "    return prefix + out + suffix\n",
    "\n",
    "def introduce_typos(text: str, prob=0.3) -> str:\n",
    "    tokens = text.split()\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        if random.random() < prob:\n",
    "            out.append(_typo_word(t))\n",
    "        else:\n",
    "            out.append(t)\n",
    "    return \" \".join(out)\n",
    "\n",
    "# Insert noise token\n",
    "def insert_noise_token(text: str):\n",
    "    tokens = text.split()\n",
    "    if len(tokens) < 3: return text\n",
    "    idx = random.randrange(1, len(tokens))\n",
    "    if random.random() < 0.5:\n",
    "        tokens.insert(idx, random.choice(FILLERS))\n",
    "    else:\n",
    "        tokens.insert(idx, tokens[idx-1])\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Truncate tail\n",
    "def truncate_tail(text: str, min_words=5, max_drop_ratio=0.4):\n",
    "    tokens = text.split()\n",
    "    n = len(tokens)\n",
    "    if n <= min_words+2: return text\n",
    "    max_drop = int(n * max_drop_ratio)\n",
    "    cut = random.randint(min_words, n - max_drop)\n",
    "    return \" \".join(tokens[:cut])\n",
    "\n",
    "# Corrupt punctuation\n",
    "def corrupt_punctuation(text: str):\n",
    "    if not re.search(r\"[,.!?]\", text): return text\n",
    "    mode = random.choice([\"drop\",\"swap\"])\n",
    "    if mode == \"drop\":\n",
    "        return re.sub(r\"[,.!?]\",\"\", text, count=random.randint(1,3))\n",
    "    else:\n",
    "        t = text.replace(\"...\",\".\")\n",
    "        t = re.sub(r\"\\?\",\".\", t)\n",
    "        return re.sub(r\"\\.\",\",\", t, count=1)\n",
    "\n",
    "# Strong degrade function\n",
    "def degrade_mid(text: str) -> str:\n",
    "    ops = [\n",
    "        lambda s: replace_one_of_set(s, MONTHS),\n",
    "        lambda s: replace_one_of_set(s, DAYS),\n",
    "        mutate_numbers,\n",
    "        flip_ampm,\n",
    "        insert_or_remove_not,\n",
    "        drop_content_words,\n",
    "        shuffle_chunks,\n",
    "        introduce_typos,\n",
    "        insert_noise_token,\n",
    "        corrupt_punctuation,\n",
    "        truncate_tail,\n",
    "    ]\n",
    "\n",
    "    random.shuffle(ops)\n",
    "    n_tokens = len(text.split())\n",
    "    base = 4 if n_tokens < 10 else 5\n",
    "    extra = 1 if n_tokens < 20 else 2\n",
    "    num_ops = min(len(ops), base + random.randint(0,extra))\n",
    "\n",
    "    out = text\n",
    "    for fn in ops[:num_ops]:\n",
    "        out = fn(out)\n",
    "\n",
    "    if out.strip() == text.strip():\n",
    "        out = drop_content_words(out, drop_ratio=(0.30,0.50))\n",
    "\n",
    "    return re.sub(r\"\\s+\",\" \", out).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b2c02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "def make_dataset(sources, targets, include_perfect=True, seed=42):\n",
    "    random.seed(seed)\n",
    "    n = len(sources)\n",
    "    rows = []\n",
    "\n",
    "    perm = list(range(n))\n",
    "    random.shuffle(perm)\n",
    "    for i in range(n):\n",
    "        if perm[i] == i:\n",
    "            j = (i+1) % n\n",
    "            perm[i], perm[j] = perm[j], perm[i]\n",
    "\n",
    "    for i in range(n):\n",
    "        src = sources[i].strip()\n",
    "        good = targets[i].strip()\n",
    "\n",
    "        if include_perfect:\n",
    "            rows.append([src, good, 100])\n",
    "\n",
    "        mid = degrade_mid(good)\n",
    "        if len(mid.split()) <= max(2, len(good.split())//4):\n",
    "            mid = degrade_mid(good)\n",
    "        rows.append([src, mid, 50])\n",
    "\n",
    "        wrong = targets[perm[i]].strip()\n",
    "        rows.append([src, wrong, 0])\n",
    "\n",
    "    return rows\n",
    "\n",
    "rows = make_dataset(srcs, tgts, include_perfect=not args.no_perfect, seed=args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a17a2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generation complete.\n"
     ]
    }
   ],
   "source": [
    "# Write CSV\n",
    "def write_csv(rows, out_path: str):\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"SOURCE\",\"TARGET\",\"SCORE\"])\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "\n",
    "write_csv(rows, args.out)\n",
    "\n",
    "print(\"Dataset generation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
