{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19219a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd50dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c:\\\\Users\\\\pockg\\\\OneDrive\\\\Desktop\\\\New',\n",
       " 'c:\\\\Users\\\\pockg\\\\OneDrive\\\\Desktop\\\\New\\\\dataset')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATASET_DIR = BASE_DIR + '\\\\dataset'\n",
    "BASE_DIR, DATASET_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbabc9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] All text files has been combined into 'source.txt'.\n",
      "[OK] All text files has been combined into 'target.txt'.\n"
     ]
    }
   ],
   "source": [
    "def merge_text_files(folder_path, output_file=\"merged.txt\", encoding=\"utf-8\"):\n",
    "    with open(output_file, \"w\", encoding=encoding) as outfile:\n",
    "        for filename in sorted(os.listdir(folder_path)):\n",
    "            if filename.lower().endswith(\".txt\"):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, \"r\", encoding=encoding) as infile:\n",
    "                    content = infile.read().strip()\n",
    "                    outfile.write(content + \"\\n\")\n",
    "    print(f\"[OK] All text files has been combined into '{output_file}'.\")\n",
    "\n",
    "source = DATASET_DIR+'\\\\ko\\\\'\n",
    "src_path = BASE_DIR+'\\\\source.txt'\n",
    "if not os.path.exists(src_path):\n",
    "    merge_text_files(source, output_file='source.txt')\n",
    "else:\n",
    "    print(\"The source file already exists.\")\n",
    "    print(f'- file path: {src_path}')\n",
    "\n",
    "target = DATASET_DIR+'\\\\en\\\\'\n",
    "tgt_path = BASE_DIR+'\\\\target.txt'\n",
    "if not os.path.exists(BASE_DIR+'\\\\target.txt'):\n",
    "    merge_text_files(target, output_file='target.txt')\n",
    "else:\n",
    "    print(\"The target file already exists.\")\n",
    "    print(f'- file path: {tgt_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0271ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = argparse.ArgumentParser(description=\"Build synthetic QE dataset CSV\")\n",
    "p.add_argument(\"--source\", default=\"source.txt\", help=\"UTF-8 Korean source file\")\n",
    "p.add_argument(\"--target\", default=\"target.txt\", help=\"UTF-8 English target file\")\n",
    "p.add_argument(\"--out\", default=\"dataset.csv\", help=\"Output CSV path\")\n",
    "p.add_argument(\"--seed\", type=int, default=42)\n",
    "p.add_argument(\"--no-perfect\", action=\"store_true\", help=\"Do NOT include 100-point rows\")\n",
    "\n",
    "args, _ = p.parse_known_args()\n",
    "if not os.path.exists(args.source) or not os.path.exists(args.target):\n",
    "    raise FileNotFoundError(\"Check the file source.txt / target.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dfa58a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8030, 8030)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_lines(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [ln.rstrip(\"\\n\\r\") for ln in f.readlines()]\n",
    "\n",
    "srcs = read_lines(args.source)\n",
    "tgts = read_lines(args.target)\n",
    "if len(srcs) != len(tgts):\n",
    "    raise ValueError(\"Both files are not matching each other.\")\n",
    "len(srcs), len(tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06ccc7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Text helpers ----------\n",
    "_WORD_RE = re.compile(r\"[^\\W\\d_]+\", re.UNICODE)  # Extract unicode words\n",
    "_NUM_RE = re.compile(r\"\\b\\d{1,4}\\b\")\n",
    "_AMPM_RE = re.compile(r\"\\b(am|pm|a\\.m\\.|p\\.m\\.)\\b\", re.IGNORECASE)\n",
    "\n",
    "MONTHS = [\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
    "          \"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]\n",
    "DAYS = [\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\"]\n",
    "\n",
    "AUX = [\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
    "       \"do\",\"does\",\"did\",\"have\",\"has\",\"had\",\n",
    "       \"can\",\"could\",\"will\",\"would\",\"shall\",\"should\",\"may\",\"might\",\"must\"]\n",
    "\n",
    "STOP_LITE = {\"a\",\"an\",\"the\",\"to\",\"of\",\"in\",\"on\",\"for\",\"and\",\"or\",\"but\",\"so\",\n",
    "             \"as\",\"at\",\"by\",\"with\",\"from\",\"than\",\"then\",\"there\",\"here\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "511ee2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preserve_case(src_token: str, repl: str) -> str:\n",
    "    if src_token.isupper():\n",
    "        return repl.upper()\n",
    "    if src_token[0].isupper():\n",
    "        return repl.capitalize()\n",
    "    return repl\n",
    "\n",
    "def replace_one_of_set(text: str, vocab: list[str]) -> str:\n",
    "    # Replace with another item only once by word boundary\n",
    "    lowered = text.lower()\n",
    "    candidates = [w for w in vocab if re.search(rf\"\\b{re.escape(w)}\\b\", lowered)]\n",
    "    if not candidates:\n",
    "        return text\n",
    "    target = random.choice(candidates)\n",
    "    others = [w for w in vocab if w != target]\n",
    "    if not others:\n",
    "        return text\n",
    "    repl = random.choice(others)\n",
    "\n",
    "    def _sub(m):\n",
    "        tok = m.group(0)\n",
    "        return _preserve_case(tok, repl)\n",
    "    return re.sub(rf\"\\b{re.escape(target)}\\b\", _sub, text, count=1, flags=re.IGNORECASE)\n",
    "\n",
    "def mutate_numbers(text: str) -> str:\n",
    "    # Damage 1 to 4 digits to +-1 or random approximation\n",
    "    def _mut(m):\n",
    "        n = int(m.group(0))\n",
    "        if n <= 2:\n",
    "            return str(n + 1)\n",
    "        if random.random() < 0.5:\n",
    "            return str(max(0, n - 1))\n",
    "        return str(n + 1)\n",
    "    return _NUM_RE.sub(_mut, text, count=random.randint(1, 2))\n",
    "\n",
    "def flip_ampm(text: str) -> str:\n",
    "    def _flip(m):\n",
    "        v = m.group(0)\n",
    "        low = v.lower()\n",
    "        alt = \"pm\" if \"am\" in low else \"am\"\n",
    "        if \".\" in low:\n",
    "            alt = alt[0] + \".m.\"\n",
    "        return _preserve_case(v, alt)\n",
    "    return _AMPM_RE.sub(_flip, text, count=1)\n",
    "\n",
    "def insert_or_remove_not(text: str) -> str:\n",
    "    # Insert 'not' after an auxiliary verb or remove an existing 'not'\n",
    "    if re.search(r\"\\bnot\\b\", text, flags=re.IGNORECASE) and random.random() < 0.5:\n",
    "        return re.sub(r\"\\bnot\\b\", \"\", text, count=1, flags=re.IGNORECASE).replace(\"  \", \" \")\n",
    "\n",
    "    # Insert\n",
    "    for aux in AUX:\n",
    "        m = re.search(rf\"\\b{aux}\\b\", text, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            i = m.end()\n",
    "            return text[:i] + \" not\" + text[i:]\n",
    "    return text\n",
    "\n",
    "def drop_content_words(text: str, drop_ratio=(0.15, 0.35)) -> str:\n",
    "    # Remove some terminology (keep short/non-word), reassemble blank criteria    \n",
    "    tokens = text.split()\n",
    "    if not tokens:\n",
    "        return text\n",
    "    k = random.uniform(*drop_ratio)\n",
    "    kept = []\n",
    "    for t in tokens:\n",
    "        plain = re.sub(r\"[^\\w]\", \"\", t, flags=re.UNICODE)\n",
    "        is_word = bool(_WORD_RE.fullmatch(plain)) and plain\n",
    "        if is_word and len(plain) > 3 and plain.lower() not in STOP_LITE:\n",
    "            if random.random() < k:\n",
    "                continue\n",
    "        kept.append(t)\n",
    "    out = \" \".join(kept)\n",
    "    return out if out.strip() else text  # If entire is removed, keep it origin.\n",
    "\n",
    "def shuffle_chunks(text: str) -> str:\n",
    "    # Mix sequences, with 2~3 chunks based on comma/adjungtive. \n",
    "    parts = re.split(r\"(,|;| and | but )\", text)\n",
    "    if len(parts) < 3:\n",
    "        toks = text.split()\n",
    "        random.shuffle(toks)\n",
    "        return \" \".join(toks)\n",
    "    \n",
    "    # Combine small parts\n",
    "    chunks, cur = [], \"\"\n",
    "    for p in parts:\n",
    "        cur += p\n",
    "        if p in {\",\",\";\",\" and \",\" but \"}:\n",
    "            chunks.append(cur.strip())\n",
    "            cur = \"\"\n",
    "    if cur.strip():\n",
    "        chunks.append(cur.strip())\n",
    "    random.shuffle(chunks)\n",
    "    return \" \".join(chunks)\n",
    "\n",
    "def degrade_mid(text: str) -> str:\n",
    "    # Create sentences which have about 50 score, with demage on rules.\n",
    "    ops = [\n",
    "        lambda s: replace_one_of_set(s, MONTHS),\n",
    "        lambda s: replace_one_of_set(s, DAYS),\n",
    "        mutate_numbers,\n",
    "        flip_ampm,\n",
    "        insert_or_remove_not,\n",
    "        drop_content_words,\n",
    "        shuffle_chunks,\n",
    "    ]\n",
    "    random.shuffle(ops)\n",
    "    out = text\n",
    "\n",
    "    # apply 3~5 things\n",
    "    for fn in ops[:random.randint(3, 5)]:\n",
    "        out = fn(out)\n",
    "\n",
    "    # If there's no change, try one more dropping.\n",
    "    if out.strip() == text.strip():\n",
    "        out = drop_content_words(out, drop_ratio=(0.30, 0.45))\n",
    "    return re.sub(r\"\\s+\", \" \", out).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b2c02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Dataset building ----------\n",
    "def make_dataset(sources, targets,\n",
    "                 include_perfect = True,\n",
    "                 seed = 42):\n",
    "    random.seed(seed)\n",
    "    n = min(len(sources), len(targets))\n",
    "    rows = []\n",
    "\n",
    "    # Index permutation for miss match(score Zero)\n",
    "    perm = list(range(n))\n",
    "    random.shuffle(perm)\n",
    "    for i in range(n):\n",
    "        if perm[i] == i:\n",
    "            j = (i + 1) % n\n",
    "            perm[i], perm[j] = perm[j], perm[i]\n",
    "\n",
    "    for i in range(n):\n",
    "        src = sources[i].strip()\n",
    "        good = targets[i].strip()\n",
    "\n",
    "        if include_perfect:\n",
    "            rows.append([src, good, 100])\n",
    "\n",
    "        # Score 50: partial damage to meaning\n",
    "        mid = degrade_mid(good)\n",
    "        if len(mid.split()) <= max(2, len(good.split()) // 4):\n",
    "            # If it's getting too short, try one more demage.\n",
    "            mid = degrade_mid(good)\n",
    "        rows.append([src, mid, 50])\n",
    "\n",
    "        # Zero: Other line's sentence.\n",
    "        wrong = targets[perm[i]].strip()\n",
    "        rows.append([src, wrong, 0])\n",
    "\n",
    "    return rows\n",
    "\n",
    "rows = make_dataset(\n",
    "    srcs, tgts,\n",
    "    include_perfect=not args.no_perfect,\n",
    "    seed=args.seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a17a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(rows, out_path: str):\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"SOURCE\", \"TARGET\", \"SCORE\"])\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "\n",
    "write_csv(rows, args.out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
