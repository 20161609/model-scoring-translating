{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f226e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909ef47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(csv='dataset.csv', model='xlm-roberta-base', epochs=3, batch_size=8, lr=1e-05, max_length=256, out_dir='./qe-model', seed=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--csv\", default=\"dataset.csv\")\n",
    "parser.add_argument(\"--model\", default=\"xlm-roberta-base\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=8)\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--max_length\", type=int, default=256)\n",
    "parser.add_argument(\"--out_dir\", default=\"./qe-model\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3fb349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading csv: dataset.csv\n",
      "[INFO] rows: 50\n",
      "[INFO] train: 45, valid: 5\n",
      "tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def load_df(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    needed = {\"SOURCE\", \"TARGET\", \"SCORE\"}\n",
    "    if not needed.issubset(df.columns):\n",
    "        raise ValueError(f\"dataset.csv에는 {needed} 컬럼이 필요함\")\n",
    "    df = df[[\"SOURCE\", \"TARGET\", \"SCORE\"]].dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def split_by_source(df, test_size=0.1, seed=42):\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    groups = df[\"SOURCE\"].values\n",
    "    idx_train, idx_valid = next(gss.split(df, groups=groups))\n",
    "    return (\n",
    "        df.iloc[idx_train].reset_index(drop=True),\n",
    "        df.iloc[idx_valid].reset_index(drop=True),\n",
    "    )\n",
    "\n",
    "set_seed(args.seed)\n",
    "print(f\"[INFO] loading csv: {args.csv}\")\n",
    "df = load_df(args.csv)\n",
    "\n",
    "print(f\"[INFO] rows: {len(df)}\")\n",
    "df_train, df_valid = split_by_source(df)\n",
    "print(f\"[INFO] train: {len(df_train)}, valid: {len(df_valid)}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "print('tokenizer loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a67bd8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded tokenizer and model 'xlm-roberta-base' for QE regression task.\n",
      "[INFO] Dataset ready: 45 training samples, 5 validation samples.\n",
      "[INFO] Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "class QEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=256):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        src, tgt = str(row[\"SOURCE\"]), str(row[\"TARGET\"])\n",
    "        label = np.clip(float(row[\"SCORE\"]) / 100.0, 0.01, 0.99)\n",
    "        text = f\"ko: {src} en: {tgt}\"\n",
    "        enc = self.tokenizer(\n",
    "            text, max_length=self.max_length, truncation=True\n",
    "        )\n",
    "        enc[\"labels\"] = torch.tensor(label, dtype=torch.float)\n",
    "        return enc\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    args.model, num_labels=1, problem_type=\"regression\"\n",
    ")\n",
    "print(f\"[INFO] Loaded tokenizer and model '{args.model}' for QE regression task.\")\n",
    "\n",
    "train_ds = QEDataset(df_train, tokenizer, args.max_length)\n",
    "valid_ds = QEDataset(df_valid, tokenizer, args.max_length)\n",
    "print(f\"[INFO] Dataset ready: {len(train_ds)} training samples, {len(valid_ds)} validation samples.\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"[INFO] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0d834f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pockg\\AppData\\Local\\Temp\\ipykernel_17592\\4093277931.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] start training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pockg\\OneDrive\\Desktop\\New\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 00:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.152909</td>\n",
       "      <td>33.995209</td>\n",
       "      <td>39.103580</td>\n",
       "      <td>0.691513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training done\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics_builder():\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        preds = preds.reshape(-1)\n",
    "        preds = np.clip(preds, 0.0, 1.0) * 100\n",
    "        labels = np.clip(labels, 0.0, 1.0) * 100\n",
    "        mae = float(np.mean(np.abs(preds - labels)))\n",
    "        rmse = float(np.sqrt(np.mean((preds - labels) ** 2)))\n",
    "        pearson = (\n",
    "            float(np.corrcoef(preds, labels)[0, 1])\n",
    "            if np.std(preds) and np.std(labels)\n",
    "            else 0.0\n",
    "        )\n",
    "        return {\"mae\": mae, \"rmse\": rmse, \"pearson\": pearson}\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=args.out_dir,\n",
    "    learning_rate=args.lr,\n",
    "    per_device_train_batch_size=args.batch_size,\n",
    "    per_device_eval_batch_size=args.batch_size * 2,\n",
    "    num_train_epochs=args.epochs,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mae\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_builder(),\n",
    ")\n",
    "\n",
    "print(\"[INFO] start training ...\")\n",
    "trainer.train()\n",
    "print(\"[INFO] training done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e2569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "SRC : 나는 클라이언트 피드백을 반영해서 다시 올리겠습니다.\n",
      "TGT : I'd like to receive a copy of my bank statement in English.\n",
      "GT  : 0\n",
      "PRED: 56.52\n",
      "----\n",
      "SRC : 그녀는 음식을 포장해 달라고 했어요.\n",
      "TGT : Can I get a refund?\n",
      "GT  : 0\n",
      "PRED: 56.22\n",
      "----\n",
      "SRC : 그는 주로 집에서 일해.\n",
      "TGT : When is your flight?\n",
      "GT  : 0\n",
      "PRED: 56.27\n",
      "----\n",
      "SRC : 너는 팀장님께 먼저 여쭤봤니?\n",
      "TGT : He turned on the navigation.\n",
      "GT  : 0\n",
      "PRED: 56.0\n",
      "----\n",
      "SRC : 나는 예산안을 다시 검토해야 합니다.\n",
      "TGT : Are you free this week?\n",
      "GT  : 100\n",
      "PRED: 56.24\n",
      "[INFO] best model saved under: ./qe-test\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Quick sanity check\n",
    "# -----------------------\n",
    "ex = df.sample(5, random_state=args.seed)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _, row in ex.iterrows():\n",
    "        text = f\"ko: {row['SOURCE']} en: {row['TARGET']}\"\n",
    "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        out = model(**enc)\n",
    "        pred = torch.sigmoid(out.logits).squeeze().item()\n",
    "        pred = np.clip(pred, 0.0, 1.0) * 100\n",
    "        print(\"----\")\n",
    "        print(\"SRC :\", row['SOURCE'])\n",
    "        print(\"TGT :\", row['TARGET'])\n",
    "        print(\"GT  :\", row['SCORE'])\n",
    "        print(\"PRED:\", round(pred, 2))\n",
    "\n",
    "print(f\"[INFO] best model saved under: {args.out_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
